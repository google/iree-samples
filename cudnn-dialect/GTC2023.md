# Compiler-cuDNN integration from first principles

**ðŸš¨ Development has moved to openxla/openxla-nvgpu. ðŸš¨**

Directory retained as was referenced in talk, but for all active work see
https://github.com/openxla/openxla-nvgpu.

Traditionally deep learning compilers have implemented convolutions on NVIDIA
GPUs in one of two ways: through code generation or by calling out to libraries
like cuDNN.  This used to be a binary choice with sharp trade-offs -- code
generation exposed opportunities for bespoke fusion but didn't utilize the
highly optimized kernels in cuDNN; and calling out to cuDNN would use highly
optimized kernels but only exploited the limited fusion opportunities exposed by
the cuDNN API.

cuDNN 8.0 changed this by introducing a "Graph API".

The cuDNN Graph API provides a declarative programming model for describing and
executing a graph of operations. Unlike the fixed set of fusion patterns the
legacy cuDNN APIs provided, these graphs can represent bespoke fusions.

In our [GTC talk](https://www.nvidia.com/gtc/session-catalog/#/session/1666649153910001zb6Z),
we introduced a clean integration of the cuDNN graph API as an
[MLIR](https://mlir.llvm.org/) dialect and a lowering from the Tensor Compute
Primitives (TCP) dialect to this cuDNN dialect. We show an end-to-end flow of
our conception of a first-class compiler integration with this API.

Through this integration, we blur the line between compilers and libraries and
enable compilers to benefit from the optimized kernels in cuDNN without giving
up fusion opportunities.


# Pointers to code

* cuDNN dialect is being [incubated in the OpenXLA project](https://github.com/iree-org/iree-samples/cudnn-dialect)
and we are developing towards graduating.

* TCP is being developed in the
[mlir-tcp](https://github.com/llvm/torch-mlir/tree/mlir-tcp) branch of
TorchMLIR. Some of the updates to lower to cuDNN are yet to land in this
branch.

# Current status

We have an MLIR pipeline that performs the following lowering:

```mermaid
flowchart LR
    id1(Torch) --> id2(TCP) --> id3(cuDNN) --> id4(LLVM);
    id2 --> id6{{cuDNN / Linalg}} --> id4;      
    id2 --> id5(Linalg) --> id4;  
```

The runtime part needed to execute cuDNN is being actively worked on.

Here we show an example of the kind of cuDNN integration that we can do with
this work. Lets start with the following ops from resnet18:

```mlir
%109 = torch.aten.relu %108
        : !torch.vtensor<[1,64,56,56],f32> -> !torch.vtensor<[1,64,56,56],f32>
%110 = torch.aten.convolution %109, %88, %none, %101, %101, %101, %false, %102, %int1
        : !torch.vtensor<[1,64,56,56],f32>, !torch.vtensor<[64,64,3,3],f32>, !torch.none,
          !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>,
          !torch.int -> !torch.vtensor<[1,64,56,56],f32>
```

They get lowered to the following TCP ops

```mlir
%139 = tcp.clamp %138 {min_float = 0.000000e+00 : f32}
        : tensor<1x64x56x56xf32> -> tensor<1x64x56x56xf32>
%140 = tcp.conv2d %139, %110 
        {dilation = array<i64: 1, 1>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}
        : tensor<1x64x56x56xf32>, tensor<64x64x3x3xf32> -> tensor<1x64x56x56xf32>
```

These ops are then fused into a group in TCP with fusion specifically to target cuDNN

```mlir
%40 = tcp.isolated_group %39, %15 {
^bb0(%arg1: tensor<1x64x56x56xf32>, %arg2: tensor<64x64x3x3xf32>):
    %171 = tcp.clamp %arg1 {min_float = 0.000000e+00 : f32}
            : tensor<1x64x56x56xf32> -> tensor<1x64x56x56xf32>
    %172 = tcp.conv2d %171, %arg2
            {dilation = array<i64: 1, 1>, pad = array<i64: 1, 1, 1, 1>, stride = array<i64: 1, 1>}
            : tensor<1x64x56x56xf32>, tensor<64x64x3x3xf32> -> tensor<1x64x56x56xf32>
    tcp.yield %172 : tensor<1x64x56x56xf32>
} : tensor<1x64x56x56xf32>, tensor<64x64x3x3xf32> -> tensor<1x64x56x56xf32>
```

The group can then be lowered to the cuDNN dialect

```mlir
%40 = cudnn.get_current_handle !cudnn.handle
%41 = cudnn.build_and_exec_graph %40 %39, %15 {
^bb0(%arg1: !cudnn.tensor_desc<1x64x56x56xf32, alignment = 1, stride = []>,
     %arg2: !cudnn.tensor_desc<64x64x3x3xf32, alignment = 1, stride = []>):
    %179 = cudnn.pointwise_relu(%arg1) type = f32 lower_clip = 0.000000e+00
            : !cudnn.tensor_desc<1x64x56x56xf32, alignment = 1, stride = []>
            -> !cudnn.tensor_desc<1x64x56x56xf32, alignment = 1, stride = []>
    %180 = cudnn.convolution(%179, %arg2)
            type = f32 alpha = 1.000000e+00 beta = 0.000000e+00 spatial_dim_count = 4
            spatial_stride = [1, 1] pre_padding = [1, 1] post_padding = [1, 1]
            dilation = [1, 1]
            : !cudnn.tensor_desc<1x64x56x56xf32, alignment = 1, stride = []>,
              !cudnn.tensor_desc<64x64x3x3xf32, alignment = 1, stride = []>
            -> !cudnn.tensor_desc<1x64x56x56xf32, alignment = 1, stride = []>
    cudnn.build_graph(%180) : !cudnn.tensor_desc<1x64x56x56xf32, alignment = 1, stride = []>
} : tensor<1x64x56x56xf32>, tensor<64x64x3x3xf32> -> tensor<1x64x56x56xf32>
```

From the cuDNN dialect the lowering in development here lowers to a function
in LLVM dialect that, given a `cudnnHandle_t` returns a finalized operation
graph descriptor using the cuDNN backend API. Further orchestration to
integrate with backend (finding execution plan, allocation workspace &
executing) is not included yet as part of this lowering.

The interface between the compiled code (commonly by way of LinAlg) and cuDNN
dialect is via MLIR Memref API and consistent bufferization/layout
transformations to required in memory representation.

# Communication channels

This project is under active development and we welcome contributions and
discussion.

* Discord for [cuDNN](https://discord.gg/HeGBk8Jk) dialect
* Discord for [TCP](https://discord.gg/DjwgYMnj) dialect
